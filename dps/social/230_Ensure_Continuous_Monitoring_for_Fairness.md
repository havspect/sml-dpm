---
title: Ensure Continuous (Human) Monitoring for Fairness
lang: en-US
sustainability-dimension: Social
ml-development-phase: Deployment and Monitoring
ml-development-stakeholders: 
    - B
    - S
    - AT
written-by: havspect
---

<script setup>
import DPOverview from '../../components/DPOverview.vue'
</script>


# {{ $frontmatter.title }} <Badge type="tip">new</Badge>

## Overview
<DPOverview />

## Description
The DP “Ensure Continuous (Human) Monitoring for Fairness” entails the continuous monitoring of ML model predictions and decisions in the real-world environment (Fahse et al., 2021). This can be achieved by establishing a continuous process to evaluate the ML model for fairness of predictions as new data is integrated, and the ML model is retrained (Burkhardt et al., 2019). Additionally, interrogating ML model decisions for plausibility by humans in fixed intervals can be considered (van Giffen et al., 2022). 

## Sources 
- Fahse, T., Huber, V., & Van Giffen, B. (2021). Managing Bias in Machine Learning Projects. Innovation Through Information Systems, 47, 94–109. https://doi.org/10.1007/978-3-030-86797-3_7
- Burkhardt. (2019). Leading your organization to responsible AI | McKinsey. https://www.mckinsey.com/capabilities/quantumblack/our-insights/leading-your-organization-to-responsible-ai
- Van Giffen, B., Herhausen, D., & Fahse, T. (2022). Overcoming the pitfalls and perils of algorithms: A classification of machine learning biases and mitigation methods. Journal of Business Research, 144, 93–106. https://doi.org/10.1016/j.jbusres.2022.01.076
